---
layout: post
use_math: true
title: "Tacotron 논문 정리"
date: 2018-12-11 18:23:10
tagline: "구글에서 개발한 음성합성 모델인 Tacotron을 스터디하여 정리"
categories:
- Speech Synthesis
tags:
- speech
- deep learning
image: /thumbnail-mobile.png
author: "Hyungcheol Noh"
permalink: /2018-12-11-tacotron
---

이번 포스팅은 다음의 논문을 스터디하여 정리하였다:
- [링크1](https://arxiv.org/abs/1703.10135)

## Tacotron?
2018년 구글에서 새로운 TTS 모델인 [Tacotron 2](https://ai.googleblog.com/2017/12/tacotron-2-generating-human-like-speech.html)를 발표하였다. 구글이 자신들의 블로그에서 주장하기를 거의 사람이 직접 내는 음성과 비슷한 퀄리티의 음성을 생성할 수 있는 모델이라고 홍보하고 있는데 실제로 Tacotron 2가 생성한 음성 샘플을 들어보면 사람의 목소리인지 구분하기 힘들 정도다. Tacotron 2의 음성 샘플은 [링크](https://google.github.io/tacotron/publications/tacotron2/index.html)를 통해서 들어볼 수 있다.

Tacotron 2에 앞서서 먼저 발표된 모델이 2017년에 발표된 Tacotron이다. 저자들은 Tacotron은 이전 TTS 모델들과 비교해서 다음과 같은 장점이 있다고 주장하고 있다.
- 텍스트를 입력으로 받아서 Raw Spectrogram을 바로 생성할 수 있음
- 간단하게 <text, audio> 페어를 이용하여 End-to-End로 학습이 가능함

![](/assets/img/2018-11-27-batch-normalization/01.png)

특히 이러한 End-to-End로 학습할 수 있다는 점은 여러 측면에서 좋은 장점을 가질 수 있다. 먼저 End-to-End가 아닌 경우에는 다음과 같은 문제점이 있을 수 있다.
- 방대한 Domain 전문 지식, 여기서는 음성에 대한 전문 지식들이 필요하다.
- 디자인에 어려움이 있을 수 있다.
- 트레이닝이 파이프라인 별로 따로 되면서 에러가 누적될 수 있고 복잡하다.

결과적으로 End-to-End로 학습하는 경우의 장점은 다음과 같이 정리할 수 있다.
- <text, audio> 페어로만 가지고 학습이 가능하다.
- Feature Engineering이 간단하다. 즉, 어떤 Feature를 뽑아서 넘겨주고 해야할지에 대한 디자인이 간단하다.
- 발화자, 언어, 감정 등의 Feature 등을 손쉽게 조절이 가능하다.
- 새로운 데이터에 더 Adaptable하다.
- 노이즈에 더 강하다.

위와 같은 주장들이 타당성이 있는 이유를 살펴보기 위해서는 기존 모델들의 특징을 한 번 살펴볼 필요가 있다.

## 기존의 TTS 모델
WaveNet (2016):
- 매우 강력한 오디오 생성 모델이다. 성능이 매우 좋아서 Tacotron의 Vocoder로 사용된다.
- 샘플 수준의 Autoregressive Model이라는 이유로 너무 느리다는 단점이 있다.
- TTS로 바로 활용할 수 없으며 TTS-Frontend로부터 Linguistic Feature를 입력으로 넣어줘야 하는 단점이 있다.

DeepVoice (2017):
- 각각의 TTS 파이프라인을 뉴럴넷 모델로 대체하였다.
- 학습이 End-to-End로 되지 않는다는 단점이 있다.

또한 최근에는 기계 번역에서 사용되었던 Encoder-Decoder 모델을 이용하여 음성 합성을 시도하는 연구들이 있었다. 2016년에 나온 모델에서는 Attention Mechanism을 이용하고 있으나 미리 학습된 HMM 모델이 Attention Mechanism으로 존재해야 하며 출력이 Audio 자체가 아닌 Vocoder 파라미터를 예측하는 방식이다. 따라서 Vocoder의 성능에 따라 결과가 달라지게 된다.

또한 최근의 Char2Wav 모델은 End-to-End로 학습하게 하였으나 여전히 Vocoder 파라미터를 예측하는 모델이라는 한계점이 있었다.

## Encoder-Decoder 모델
![](/assets/img/2018-11-27-batch-normalization/01.png)

그림은 기본적인 Encoder-Decoder 모델을 나타낸 것이다. Encoder-Decoder 모델은 기계 번역에서 처음으로 사용하기 시작한 모델이다. Encoder와 Decoder를 각각 RNN(그림에서는 LSTM을 사용했지만) 셀로 구성한다. Encoder는 순차적으로 번역하고자 하는 문장의 시퀀스를 입력으로 받게 되고 문장을 고정된 길이의 벡터로 임베딩을 하게 된다. RNN 인코더의 경우 입력 시퀀스의 마지막 타임 스텝의 입력, 여기서는 <EOS> (End of sequence)를 입력으로 넣었을 때 나오는 RNN 셀의 Hidden State Vector를 문장의 임베딩으로 사용하게 된다. 이렇게 생성된 문장 임베딩은 다시 RNN 디코더의 첫번째 타임 스텝의 입력으로 들어가게 되고 디코더는 순차적으로 문장을 생성하여 결국 번역된 문장을 완성하게 된다.
  
간단하게 정리하면 현재 타임 스텝의 디코더 출력 $$y_i$$를 생성하기 위해서는 인코더에 입력되는 문장 시퀀스 $$\mathbf{x}$$와 이전 타임 스텝들의 디코더 출력 시퀀스 $$\{ y_1, \cdots, y_{i-1}\}$$이 Conditioning되는 조건부 확률 $$p(y_i \vert y_1,\cdots, y_{i-1}, \mathbf{x})$$는 RNN Encoder-Decoder 모델에서 다음과 같이 모델링될 수 있다.

$$
\begin{align*}
p(y_i \vert y_1,\cdots, y_{i-1}, \mathbf{x}) & = g(y_{i-1}, s_{i-1}) \\
s_i & = f(y_{i-1}, s_{i-1}) \\
\end{align*}
$$

$$
\begin{align*}
\text{where} \ y_0 = \text{Enc}(\mathbf{x})
\end{align*}
$$
  
![](/assets/img/2018-11-27-batch-normalization/01.png)

그 다음 그림은 기본 Encoder-Decoder 모델에 Attention Mechanism을 추가한 것을 나타낸다. Attention Mechanism은 문장을 하나의 고정된 길이의 벡터로 임베딩하는 것을 부정하는 과정에서 연구가 시작된 모델이다. 문장을 구성하는 모든 단어들을 각각 임베딩하여 디코더가 특정 타임 스텝에서 어떤 단어에 더 집중하여 현재 타임 스텝의 출력을 생성할지에 Attention Mechanism이 도움을 주게 된다. Tacotron은 Attention Mechanism으로 Bahdanau Attention을 사용하였으며 Bahdanau Attention에 대한 정리는 이 [포스트](https://hcnoh.github.io/2018-12-11-bahdanau-attention)에서 확인할 수 있다.

간단하게 정리하자면 Bahdanau Attention 같은 경우는 이전 타임 스텝의 디코더 Hidden State Vector와 인코더의 출력인 단어 임베딩 시퀀스를 입력으로 받아서 다음 현재 타임 스텝에서의 Context Vector를 생성한다. 이렇게 생성된 Context Vector는 이전 타임 스텝의 Hidden State Vector와 함께 사용되어 디코더가 현재 타임 스텝의 출력을 생성하게 된다.

좀 더 간단하게 정리하면 다음과 같다. Attention Mechanism을 적용하는 경우 조건부 확률 $$p(y_i \vert y_1,\cdots, y_{i-1}, \mathbf{x})$$는 RNN Encoder-Decoder 모델에서 다음과 같이 모델링될 수 있다.

$$
\begin{align*}
p(y_i \vert y_1,\cdots, y_{i-1}, \mathbf{x}) & = g(y_{i-1}, s_{i-1}, c_i) \\
s_i & = f(y_{i-1}, s_{i-1}, c_i) \\
\end{align*}
$$

$$
\begin{align*}
\text{where} \ y_0 & = \text{<Go> Token}, \\
c_i & = \text{Attn}(s_{i-1}, [h_1, \cdots, h_{T_{\mathbf{x}}}])
\end{align*}
$$


