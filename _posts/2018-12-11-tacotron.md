---
layout: post
use_math: true
title: "Tacotron 논문 정리"
date: 2018-12-11 18:23:10
tagline: "구글에서 개발한 음성합성 모델인 Tacotron을 스터디하여 정리"
categories:
- Speech Synthesis
tags:
- speech
- deep learning
image: /thumbnail-mobile.png
author: "Hyungcheol Noh"
permalink: /2018-12-11-tacotron
---

이번 포스팅은 다음의 논문을 스터디하여 정리하였다:
- [링크1](https://arxiv.org/abs/1703.10135)

## Tacotron?
2018년 구글에서 새로운 TTS 모델인 [Tacotron 2](https://ai.googleblog.com/2017/12/tacotron-2-generating-human-like-speech.html)를 발표하였다. 구글이 자신들의 블로그에서 주장하기를 거의 사람이 직접 내는 음성과 비슷한 퀄리티의 음성을 생성할 수 있는 모델이라고 홍보하고 있는데 실제로 Tacotron 2가 생성한 음성 샘플을 들어보면 사람의 목소리인지 구분하기 힘들 정도다. Tacotron 2의 음성 샘플은 [링크](https://google.github.io/tacotron/publications/tacotron2/index.html)를 통해서 들어볼 수 있다.

Tacotron 2에 앞서서 먼저 발표된 모델이 2017년에 발표된 Tacotron이다. 저자들은 Tacotron은 이전 TTS 모델들과 비교해서 다음과 같은 장점이 있다고 주장하고 있다.
- 텍스트를 입력으로 받아서 Raw Spectrogram을 바로 생성할 수 있음
- 간단하게 <text, audio> 페어를 이용하여 End-to-End로 학습이 가능함

![](/assets/img/2018-11-27-batch-normalization/01.png)

특히 이러한 End-to-End로 학습할 수 있다는 점은 여러 측면에서 좋은 장점을 가질 수 있다. 먼저 End-to-End가 아닌 경우에는 다음과 같은 문제점이 있을 수 있다.
- 방대한 Domain 전문 지식, 여기서는 음성에 대한 전문 지식들이 필요하다.
- 디자인에 어려움이 있을 수 있다.
- 트레이닝이 파이프라인 별로 따로 되면서 에러가 누적될 수 있고 복잡하다.

결과적으로 End-to-End로 학습하는 경우의 장점은 다음과 같이 정리할 수 있다.
- <text, audio> 페어로만 가지고 학습이 가능하다.
- Feature Engineering이 간단하다. 즉, 어떤 Feature를 뽑아서 넘겨주고 해야할지에 대한 디자인이 간단하다.
- 발화자, 언어, 감정 등의 Feature 등을 손쉽게 조절이 가능하다.
- 새로운 데이터에 더 Adaptable하다.
- 노이즈에 더 강하다.

위와 같은 주장들이 타당성이 있는 이유를 살펴보기 위해서는 기존 모델들의 특징을 한 번 살펴볼 필요가 있다.

## 기존의 TTS 모델
WaveNet (2016):
- 매우 강력한 오디오 생성 모델이다. 성능이 매우 좋아서 Tacotron의 Vocoder로 사용된다.
- 샘플 수준의 Autoregressive Model이라는 이유로 너무 느리다는 단점이 있다.
- TTS로 바로 활용할 수 없으며 TTS-Frontend로부터 Linguistic Feature를 입력으로 넣어줘야 하는 단점이 있다.

DeepVoice (2017):
- 각각의 TTS 파이프라인을 뉴럴넷 모델로 대체하였다.
- 학습이 End-to-End로 되지 않는다는 단점이 있다.

또한 최근에는 기계 번역에서 사용되었던 Encoder-Decoder 모델을 이용하여 음성 합성을 시도하는 연구들이 있었다. 2016년에 나온 모델에서는 Attention Mechanism을 이용하고 있으나 미리 학습된 HMM 모델이 Attention Mechanism으로 존재해야 하며 출력이 Audio 자체가 아닌 Vocoder 파라미터를 예측하는 방식이다. 따라서 Vocoder의 성능에 따라 결과가 달라지게 된다.

또한 최근의 Char2Wav 모델은 End-to-End로 학습하게 하였으나 여전히 Vocoder 파라미터를 예측하는 모델이라는 한계점이 있었다.


